// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.

namespace Microsoft.Azure.Management.DataFactory.Models
{
    using System.Linq;

    /// <summary>
    /// Execute spark job activity properties.
    /// </summary>
    public partial class SynapseSparkJobActivityTypeProperties
    {
        /// <summary>
        /// Initializes a new instance of the SynapseSparkJobActivityTypeProperties class.
        /// </summary>
        public SynapseSparkJobActivityTypeProperties()
        {
            this.SparkJob = new SynapseSparkJobReference();
            CustomInit();
        }

        /// <summary>
        /// Initializes a new instance of the SynapseSparkJobActivityTypeProperties class.
        /// </summary>

        /// <param name="sparkJob">Synapse spark job reference.
        /// </param>

        /// <param name="arguments">User specified arguments to SynapseSparkJobDefinitionActivity.
        /// </param>

        /// <param name="file">The main file used for the job, which will override the &#39;file&#39; of the spark
        /// job definition you provide. Type: string (or Expression with resultType
        /// string).
        /// </param>

        /// <param name="scanFolder">Scanning subfolders from the root folder of the main definition file, these
        /// files will be added as reference files. The folders named &#39;jars&#39;,
        /// &#39;pyFiles&#39;, &#39;files&#39; or &#39;archives&#39; will be scanned, and the folders name are
        /// case sensitive. Type: boolean (or Expression with resultType boolean).
        /// </param>

        /// <param name="className">The fully-qualified identifier or the main class that is in the main
        /// definition file, which will override the &#39;className&#39; of the spark job
        /// definition you provide. Type: string (or Expression with resultType
        /// string).
        /// </param>

        /// <param name="files">(Deprecated. Please use pythonCodeReference and filesV2) Additional files
        /// used for reference in the main definition file, which will override the
        /// &#39;files&#39; of the spark job definition you provide.
        /// </param>

        /// <param name="pythonCodeReference">Additional python code files used for reference in the main definition
        /// file, which will override the &#39;pyFiles&#39; of the spark job definition you
        /// provide.
        /// </param>

        /// <param name="filesV2">Additional files used for reference in the main definition file, which will
        /// override the &#39;jars&#39; and &#39;files&#39; of the spark job definition you provide.
        /// </param>

        /// <param name="targetBigDataPool">The name of the big data pool which will be used to execute the spark batch
        /// job, which will override the &#39;targetBigDataPool&#39; of the spark job
        /// definition you provide.
        /// </param>

        /// <param name="executorSize">Number of core and memory to be used for executors allocated in the
        /// specified Spark pool for the job, which will be used for overriding
        /// &#39;executorCores&#39; and &#39;executorMemory&#39; of the spark job definition you
        /// provide. Type: string (or Expression with resultType string).
        /// </param>

        /// <param name="conf">Spark configuration properties, which will override the &#39;conf&#39; of the spark
        /// job definition you provide.
        /// </param>

        /// <param name="driverSize">Number of core and memory to be used for driver allocated in the specified
        /// Spark pool for the job, which will be used for overriding &#39;driverCores&#39; and
        /// &#39;driverMemory&#39; of the spark job definition you provide. Type: string (or
        /// Expression with resultType string).
        /// </param>

        /// <param name="numExecutors">Number of executors to launch for this job, which will override the
        /// &#39;numExecutors&#39; of the spark job definition you provide. Type: integer (or
        /// Expression with resultType integer).
        /// </param>

        /// <param name="configurationType">The type of the spark config.
        /// Possible values include: &#39;Default&#39;, &#39;Customized&#39;, &#39;Artifact&#39;</param>

        /// <param name="targetSparkConfiguration">The spark configuration of the spark job.
        /// </param>

        /// <param name="sparkConfig">Spark configuration property.
        /// </param>
        public SynapseSparkJobActivityTypeProperties(SynapseSparkJobReference sparkJob, System.Collections.Generic.IList<object> arguments = default(System.Collections.Generic.IList<object>), object file = default(object), object scanFolder = default(object), object className = default(object), System.Collections.Generic.IList<object> files = default(System.Collections.Generic.IList<object>), System.Collections.Generic.IList<object> pythonCodeReference = default(System.Collections.Generic.IList<object>), System.Collections.Generic.IList<object> filesV2 = default(System.Collections.Generic.IList<object>), BigDataPoolParametrizationReference targetBigDataPool = default(BigDataPoolParametrizationReference), object executorSize = default(object), object conf = default(object), object driverSize = default(object), object numExecutors = default(object), string configurationType = default(string), SparkConfigurationParametrizationReference targetSparkConfiguration = default(SparkConfigurationParametrizationReference), System.Collections.Generic.IDictionary<string, object> sparkConfig = default(System.Collections.Generic.IDictionary<string, object>))

        {
            this.SparkJob = sparkJob;
            this.Arguments = arguments;
            this.File = file;
            this.ScanFolder = scanFolder;
            this.ClassName = className;
            this.Files = files;
            this.PythonCodeReference = pythonCodeReference;
            this.FilesV2 = filesV2;
            this.TargetBigDataPool = targetBigDataPool;
            this.ExecutorSize = executorSize;
            this.Conf = conf;
            this.DriverSize = driverSize;
            this.NumExecutors = numExecutors;
            this.ConfigurationType = configurationType;
            this.TargetSparkConfiguration = targetSparkConfiguration;
            this.SparkConfig = sparkConfig;
            CustomInit();
        }

        /// <summary>
        /// An initialization method that performs custom operations like setting defaults
        /// </summary>
        partial void CustomInit();


        /// <summary>
        /// Gets or sets synapse spark job reference.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "sparkJob")]
        public SynapseSparkJobReference SparkJob {get; set; }

        /// <summary>
        /// Gets or sets user specified arguments to SynapseSparkJobDefinitionActivity.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "args")]
        public System.Collections.Generic.IList<object> Arguments {get; set; }

        /// <summary>
        /// Gets or sets the main file used for the job, which will override the &#39;file&#39;
        /// of the spark job definition you provide. Type: string (or Expression with
        /// resultType string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "file")]
        public object File {get; set; }

        /// <summary>
        /// Gets or sets scanning subfolders from the root folder of the main
        /// definition file, these files will be added as reference files. The folders
        /// named &#39;jars&#39;, &#39;pyFiles&#39;, &#39;files&#39; or &#39;archives&#39; will be scanned, and the
        /// folders name are case sensitive. Type: boolean (or Expression with
        /// resultType boolean).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "scanFolder")]
        public object ScanFolder {get; set; }

        /// <summary>
        /// Gets or sets the fully-qualified identifier or the main class that is in
        /// the main definition file, which will override the &#39;className&#39; of the spark
        /// job definition you provide. Type: string (or Expression with resultType
        /// string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "className")]
        public object ClassName {get; set; }

        /// <summary>
        /// Gets or sets (Deprecated. Please use pythonCodeReference and filesV2)
        /// Additional files used for reference in the main definition file, which will
        /// override the &#39;files&#39; of the spark job definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "files")]
        public System.Collections.Generic.IList<object> Files {get; set; }

        /// <summary>
        /// Gets or sets additional python code files used for reference in the main
        /// definition file, which will override the &#39;pyFiles&#39; of the spark job
        /// definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "pythonCodeReference")]
        public System.Collections.Generic.IList<object> PythonCodeReference {get; set; }

        /// <summary>
        /// Gets or sets additional files used for reference in the main definition
        /// file, which will override the &#39;jars&#39; and &#39;files&#39; of the spark job
        /// definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "filesV2")]
        public System.Collections.Generic.IList<object> FilesV2 {get; set; }

        /// <summary>
        /// Gets or sets the name of the big data pool which will be used to execute
        /// the spark batch job, which will override the &#39;targetBigDataPool&#39; of the
        /// spark job definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "targetBigDataPool")]
        public BigDataPoolParametrizationReference TargetBigDataPool {get; set; }

        /// <summary>
        /// Gets or sets number of core and memory to be used for executors allocated
        /// in the specified Spark pool for the job, which will be used for overriding
        /// &#39;executorCores&#39; and &#39;executorMemory&#39; of the spark job definition you
        /// provide. Type: string (or Expression with resultType string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "executorSize")]
        public object ExecutorSize {get; set; }

        /// <summary>
        /// Gets or sets spark configuration properties, which will override the &#39;conf&#39;
        /// of the spark job definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "conf")]
        public object Conf {get; set; }

        /// <summary>
        /// Gets or sets number of core and memory to be used for driver allocated in
        /// the specified Spark pool for the job, which will be used for overriding
        /// &#39;driverCores&#39; and &#39;driverMemory&#39; of the spark job definition you provide.
        /// Type: string (or Expression with resultType string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "driverSize")]
        public object DriverSize {get; set; }

        /// <summary>
        /// Gets or sets number of executors to launch for this job, which will
        /// override the &#39;numExecutors&#39; of the spark job definition you provide. Type:
        /// integer (or Expression with resultType integer).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "numExecutors")]
        public object NumExecutors {get; set; }

        /// <summary>
        /// Gets or sets the type of the spark config. Possible values include: &#39;Default&#39;, &#39;Customized&#39;, &#39;Artifact&#39;
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "configurationType")]
        public string ConfigurationType {get; set; }

        /// <summary>
        /// Gets or sets the spark configuration of the spark job.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "targetSparkConfiguration")]
        public SparkConfigurationParametrizationReference TargetSparkConfiguration {get; set; }

        /// <summary>
        /// Gets or sets spark configuration property.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "sparkConfig")]
        public System.Collections.Generic.IDictionary<string, object> SparkConfig {get; set; }
        /// <summary>
        /// Validate the object.
        /// </summary>
        /// <exception cref="Microsoft.Rest.ValidationException">
        /// Thrown if validation fails
        /// </exception>
        public virtual void Validate()
        {
            if (this.SparkJob == null)
            {
                throw new Microsoft.Rest.ValidationException(Microsoft.Rest.ValidationRules.CannotBeNull, "SparkJob");
            }
            if (this.SparkJob != null)
            {
                this.SparkJob.Validate();
            }







            if (this.TargetBigDataPool != null)
            {
                this.TargetBigDataPool.Validate();
            }





            if (this.TargetSparkConfiguration != null)
            {
                this.TargetSparkConfiguration.Validate();
            }

        }
    }
}