// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.
// Code generated by Microsoft (R) AutoRest Code Generator.
// Changes may cause incorrect behavior and will be lost if the code is regenerated.

namespace Microsoft.Azure.Management.DataFactory.Models
{
    using System.Linq;

    /// <summary>
    /// Execute spark job activity.
    /// </summary>
    [Newtonsoft.Json.JsonObject("SparkJob")]
    [Microsoft.Rest.Serialization.JsonTransformation]
    public partial class SynapseSparkJobDefinitionActivity : ExecutionActivity
    {
        /// <summary>
        /// Initializes a new instance of the SynapseSparkJobDefinitionActivity class.
        /// </summary>
        public SynapseSparkJobDefinitionActivity()
        {
            this.SparkJob = new SynapseSparkJobReference();
            CustomInit();
        }

        /// <summary>
        /// Initializes a new instance of the SynapseSparkJobDefinitionActivity class.
        /// </summary>

        /// <param name="additionalProperties">A pipeline activity.
        /// </param>

        /// <param name="name">Activity name.
        /// </param>

        /// <param name="description">Activity description.
        /// </param>

        /// <param name="state">Activity state. This is an optional property and if not provided, the state
        /// will be Active by default.
        /// Possible values include: &#39;Active&#39;, &#39;Inactive&#39;</param>

        /// <param name="onInactiveMarkAs">Status result of the activity when the state is set to Inactive. This is an
        /// optional property and if not provided when the activity is inactive, the
        /// status will be Succeeded by default.
        /// Possible values include: &#39;Succeeded&#39;, &#39;Failed&#39;, &#39;Skipped&#39;</param>

        /// <param name="dependsOn">Activity depends on condition.
        /// </param>

        /// <param name="userProperties">Activity user properties.
        /// </param>

        /// <param name="linkedServiceName">Linked service reference.
        /// </param>

        /// <param name="policy">Activity policy.
        /// </param>

        /// <param name="configurationType">The type of the spark config.
        /// Possible values include: &#39;Default&#39;, &#39;Customized&#39;, &#39;Artifact&#39;</param>

        /// <param name="sparkJob">Synapse spark job reference.
        /// </param>

        /// <param name="arguments">User specified arguments to SynapseSparkJobDefinitionActivity.
        /// </param>

        /// <param name="file">The main file used for the job, which will override the &#39;file&#39; of the spark
        /// job definition you provide. Type: string (or Expression with resultType
        /// string).
        /// </param>

        /// <param name="scanFolder">Scanning subfolders from the root folder of the main definition file, these
        /// files will be added as reference files. The folders named &#39;jars&#39;,
        /// &#39;pyFiles&#39;, &#39;files&#39; or &#39;archives&#39; will be scanned, and the folders name are
        /// case sensitive. Type: boolean (or Expression with resultType boolean).
        /// </param>

        /// <param name="className">The fully-qualified identifier or the main class that is in the main
        /// definition file, which will override the &#39;className&#39; of the spark job
        /// definition you provide. Type: string (or Expression with resultType
        /// string).
        /// </param>

        /// <param name="files">(Deprecated. Please use pythonCodeReference and filesV2) Additional files
        /// used for reference in the main definition file, which will override the
        /// &#39;files&#39; of the spark job definition you provide.
        /// </param>

        /// <param name="pythonCodeReference">Additional python code files used for reference in the main definition
        /// file, which will override the &#39;pyFiles&#39; of the spark job definition you
        /// provide.
        /// </param>

        /// <param name="filesV2">Additional files used for reference in the main definition file, which will
        /// override the &#39;jars&#39; and &#39;files&#39; of the spark job definition you provide.
        /// </param>

        /// <param name="targetBigDataPool">The name of the big data pool which will be used to execute the spark batch
        /// job, which will override the &#39;targetBigDataPool&#39; of the spark job
        /// definition you provide.
        /// </param>

        /// <param name="executorSize">Number of core and memory to be used for executors allocated in the
        /// specified Spark pool for the job, which will be used for overriding
        /// &#39;executorCores&#39; and &#39;executorMemory&#39; of the spark job definition you
        /// provide. Type: string (or Expression with resultType string).
        /// </param>

        /// <param name="conf">Spark configuration properties, which will override the &#39;conf&#39; of the spark
        /// job definition you provide.
        /// </param>

        /// <param name="driverSize">Number of core and memory to be used for driver allocated in the specified
        /// Spark pool for the job, which will be used for overriding &#39;driverCores&#39; and
        /// &#39;driverMemory&#39; of the spark job definition you provide. Type: string (or
        /// Expression with resultType string).
        /// </param>

        /// <param name="numExecutors">Number of executors to launch for this job, which will override the
        /// &#39;numExecutors&#39; of the spark job definition you provide. Type: integer (or
        /// Expression with resultType integer).
        /// </param>

        /// <param name="targetSparkConfiguration">The spark configuration of the spark job.
        /// </param>

        /// <param name="sparkConfig">Spark configuration property.
        /// </param>
        public SynapseSparkJobDefinitionActivity(string name, SynapseSparkJobReference sparkJob, System.Collections.Generic.IDictionary<string, object> additionalProperties = default(System.Collections.Generic.IDictionary<string, object>), string description = default(string), string state = default(string), string onInactiveMarkAs = default(string), System.Collections.Generic.IList<ActivityDependency> dependsOn = default(System.Collections.Generic.IList<ActivityDependency>), System.Collections.Generic.IList<UserProperty> userProperties = default(System.Collections.Generic.IList<UserProperty>), LinkedServiceReference linkedServiceName = default(LinkedServiceReference), ActivityPolicy policy = default(ActivityPolicy), string configurationType = default(string), System.Collections.Generic.IList<object> arguments = default(System.Collections.Generic.IList<object>), object file = default(object), object scanFolder = default(object), object className = default(object), System.Collections.Generic.IList<object> files = default(System.Collections.Generic.IList<object>), System.Collections.Generic.IList<object> pythonCodeReference = default(System.Collections.Generic.IList<object>), System.Collections.Generic.IList<object> filesV2 = default(System.Collections.Generic.IList<object>), BigDataPoolParametrizationReference targetBigDataPool = default(BigDataPoolParametrizationReference), object executorSize = default(object), object conf = default(object), object driverSize = default(object), object numExecutors = default(object), SparkConfigurationParametrizationReference targetSparkConfiguration = default(SparkConfigurationParametrizationReference), System.Collections.Generic.IDictionary<string, object> sparkConfig = default(System.Collections.Generic.IDictionary<string, object>))

        : base(name, additionalProperties, description, state, onInactiveMarkAs, dependsOn, userProperties, linkedServiceName, policy)
        {
            this.ConfigurationType = configurationType;
            this.SparkJob = sparkJob;
            this.Arguments = arguments;
            this.File = file;
            this.ScanFolder = scanFolder;
            this.ClassName = className;
            this.Files = files;
            this.PythonCodeReference = pythonCodeReference;
            this.FilesV2 = filesV2;
            this.TargetBigDataPool = targetBigDataPool;
            this.ExecutorSize = executorSize;
            this.Conf = conf;
            this.DriverSize = driverSize;
            this.NumExecutors = numExecutors;
            this.TargetSparkConfiguration = targetSparkConfiguration;
            this.SparkConfig = sparkConfig;
            CustomInit();
        }

        /// <summary>
        /// An initialization method that performs custom operations like setting defaults
        /// </summary>
        partial void CustomInit();


        /// <summary>
        /// Gets or sets the type of the spark config. Possible values include: &#39;Default&#39;, &#39;Customized&#39;, &#39;Artifact&#39;
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.configurationType")]
        public string ConfigurationType {get; set; }

        /// <summary>
        /// Gets or sets synapse spark job reference.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.sparkJob")]
        public SynapseSparkJobReference SparkJob {get; set; }

        /// <summary>
        /// Gets or sets user specified arguments to SynapseSparkJobDefinitionActivity.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.args")]
        public System.Collections.Generic.IList<object> Arguments {get; set; }

        /// <summary>
        /// Gets or sets the main file used for the job, which will override the &#39;file&#39;
        /// of the spark job definition you provide. Type: string (or Expression with
        /// resultType string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.file")]
        public object File {get; set; }

        /// <summary>
        /// Gets or sets scanning subfolders from the root folder of the main
        /// definition file, these files will be added as reference files. The folders
        /// named &#39;jars&#39;, &#39;pyFiles&#39;, &#39;files&#39; or &#39;archives&#39; will be scanned, and the
        /// folders name are case sensitive. Type: boolean (or Expression with
        /// resultType boolean).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.scanFolder")]
        public object ScanFolder {get; set; }

        /// <summary>
        /// Gets or sets the fully-qualified identifier or the main class that is in
        /// the main definition file, which will override the &#39;className&#39; of the spark
        /// job definition you provide. Type: string (or Expression with resultType
        /// string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.className")]
        public object ClassName {get; set; }

        /// <summary>
        /// Gets or sets (Deprecated. Please use pythonCodeReference and filesV2)
        /// Additional files used for reference in the main definition file, which will
        /// override the &#39;files&#39; of the spark job definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.files")]
        public System.Collections.Generic.IList<object> Files {get; set; }

        /// <summary>
        /// Gets or sets additional python code files used for reference in the main
        /// definition file, which will override the &#39;pyFiles&#39; of the spark job
        /// definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.pythonCodeReference")]
        public System.Collections.Generic.IList<object> PythonCodeReference {get; set; }

        /// <summary>
        /// Gets or sets additional files used for reference in the main definition
        /// file, which will override the &#39;jars&#39; and &#39;files&#39; of the spark job
        /// definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.filesV2")]
        public System.Collections.Generic.IList<object> FilesV2 {get; set; }

        /// <summary>
        /// Gets or sets the name of the big data pool which will be used to execute
        /// the spark batch job, which will override the &#39;targetBigDataPool&#39; of the
        /// spark job definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.targetBigDataPool")]
        public BigDataPoolParametrizationReference TargetBigDataPool {get; set; }

        /// <summary>
        /// Gets or sets number of core and memory to be used for executors allocated
        /// in the specified Spark pool for the job, which will be used for overriding
        /// &#39;executorCores&#39; and &#39;executorMemory&#39; of the spark job definition you
        /// provide. Type: string (or Expression with resultType string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.executorSize")]
        public object ExecutorSize {get; set; }

        /// <summary>
        /// Gets or sets spark configuration properties, which will override the &#39;conf&#39;
        /// of the spark job definition you provide.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.conf")]
        public object Conf {get; set; }

        /// <summary>
        /// Gets or sets number of core and memory to be used for driver allocated in
        /// the specified Spark pool for the job, which will be used for overriding
        /// &#39;driverCores&#39; and &#39;driverMemory&#39; of the spark job definition you provide.
        /// Type: string (or Expression with resultType string).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.driverSize")]
        public object DriverSize {get; set; }

        /// <summary>
        /// Gets or sets number of executors to launch for this job, which will
        /// override the &#39;numExecutors&#39; of the spark job definition you provide. Type:
        /// integer (or Expression with resultType integer).
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.numExecutors")]
        public object NumExecutors {get; set; }

        /// <summary>
        /// Gets or sets the spark configuration of the spark job.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.targetSparkConfiguration")]
        public SparkConfigurationParametrizationReference TargetSparkConfiguration {get; set; }

        /// <summary>
        /// Gets or sets spark configuration property.
        /// </summary>
        [Newtonsoft.Json.JsonProperty(PropertyName = "typeProperties.sparkConfig")]
        public System.Collections.Generic.IDictionary<string, object> SparkConfig {get; set; }
        /// <summary>
        /// Validate the object.
        /// </summary>
        /// <exception cref="Microsoft.Rest.ValidationException">
        /// Thrown if validation fails
        /// </exception>
        public override void Validate()
        {
            base.Validate();
            if (this.SparkJob == null)
            {
                throw new Microsoft.Rest.ValidationException(Microsoft.Rest.ValidationRules.CannotBeNull, "SparkJob");
            }

            if (this.SparkJob != null)
            {
                this.SparkJob.Validate();
            }







            if (this.TargetBigDataPool != null)
            {
                this.TargetBigDataPool.Validate();
            }




            if (this.TargetSparkConfiguration != null)
            {
                this.TargetSparkConfiguration.Validate();
            }

        }
    }
}