{
  "Entries": [
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults?api-version=2016-06-01",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzP2FwaS12ZXJzaW9uPTIwMTYtMDYtMDE=",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "10390bd4-b76a-4282-bbde-974073476226"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.RecoveryServicesClient/4.3.1.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-request-id": [
          "760f65f1-ff86-4b8d-b7c7-46994b6242ad"
        ],
        "x-ms-client-request-id": [
          "10390bd4-b76a-4282-bbde-974073476226"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "Server": [
          "Microsoft-IIS/10.0"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11999"
        ],
        "x-ms-correlation-request-id": [
          "760f65f1-ff86-4b8d-b7c7-46994b6242ad"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052718Z:760f65f1-ff86-4b8d-b7c7-46994b6242ad"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:18 GMT"
        ],
        "Content-Length": [
          "474"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"location\": \"centraluseuap\",\r\n      \"name\": \"PwsTestVault\",\r\n      \"etag\": \"W/\\\"datetime'2021-03-24T14%3A01%3A50.8326358Z'\\\"\",\r\n      \"properties\": {\r\n        \"provisioningState\": \"Succeeded\",\r\n        \"privateEndpointStateForBackup\": \"None\",\r\n        \"privateEndpointStateForSiteRecovery\": \"None\"\r\n      },\r\n      \"id\": \"/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults\",\r\n      \"sku\": {\r\n        \"name\": \"RS0\",\r\n        \"tier\": \"Standard\"\r\n      }\r\n    }\r\n  ]\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz9hcGktdmVyc2lvbj0yMDIxLTAyLTEw",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "44141930-9f5e-41fc-b897-6eebbc90be36"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769646016)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374446016)\\/\",\"ClientRequestId\":\"6a13174a-f8af-4bb5-acee-fc7432d88efc-2021-04-07 05:27:26Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"BrF1pIZbdJT/SqAhOsKZWuzqLRqYbJbunz/iEjTilTk=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11998"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "44141930-9f5e-41fc-b897-6eebbc90be36 4/7/2021 5:27:26 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "44141930-9f5e-41fc-b897-6eebbc90be36"
        ],
        "x-ms-correlation-request-id": [
          "eba99e8c-6783-4295-b938-84ae1ba86588"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052726Z:eba99e8c-6783-4295-b938-84ae1ba86588"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:26 GMT"
        ],
        "Content-Length": [
          "126319"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:47.661058Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:51:55Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T17:51:55Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.6243468Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.6243468Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.5843657Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.5843657Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T16:51:37.4950021Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7862644Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7862644Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_PositiveSourceClockSkewExceeded\",\r\n        \"description\": \"Operating system clock skewed.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7512693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7512693Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:35:44.8280307Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:28:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:28:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.9209352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.9209352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.8859347Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.8859347Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:18:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:18:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:44:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T10:44:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:34:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T10:34:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:23:10.229837Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:07:07.5003948Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T08:07:07.5003948Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7388078Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          },\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78222\",\r\n            \"summaryMessage\": \"Resynchronization required\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of following disk(s) to Azure - 'Disk1'.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7138109Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7138109Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0434\",\r\n        \"description\": \"VM reboot\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Server has come up at 20210405105909.500000-420 after a reboot or shutdown\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90081\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Server 'V2A-W2K19-202' has come up after a reboot or shutdown.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0435\",\r\n        \"description\": \"Agent upgrade\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Agent upgraded to v9.41.5888.1\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90082\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Mobility service upgraded to version '9.41.5888.1' on server 'V2A-W2K19-202'.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"TargetAgentInternalError\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:40:31Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"Resync Required\",\r\n          \"component\": \"MT\",\r\n          \"correctiveAction\": \"TargetAgentInternalError\",\r\n          \"details\": \"TargetAgentInternalError\",\r\n          \"summary\": \"TargetAgentInternalError{5D628CF2-6CAA-468D-9E7A-35881C0D00DE}\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90099\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of disk ('Disk1') to Azure.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:40:31Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.5597108Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0722095Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0722095Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0422138Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0422138Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:01:33Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:01:33Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:51:32Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K19-202:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:51:32Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T05:11:04.8858693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:57:15Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T04:57:15Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3816352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3816352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3516376Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3516376Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:47:14Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:47:14Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:46:14.4606026Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.5197938Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.5197938Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.489793Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.489793Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:18:49Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T02:18:49Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4664319Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4664319Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4114289Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4114289Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"e1a52cf6-cdc5-4a8c-913b-0447da3b024d\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T01:56:50Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T01:56:50Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090755578814775807_c81ebebe-b9d1-4401-821d-5610f0a30bc9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090755578814775807_c81ebebe-b9d1-4401-821d-5610f0a30bc9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"5169c78a-7f64-430d-acbd-0318cb50b565\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-03-31T06:30:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-03-31T06:30:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=Severity%20eq%20'OK'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPVNldmVyaXR5JTIwZXElMjAnT0snJmFwaS12ZXJzaW9uPTIwMjEtMDItMTA=",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "3e1c1c67-8e00-4641-b7c1-f2f18b8617dc"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769648205)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374448205)\\/\",\"ClientRequestId\":\"e3551cef-9174-4089-91f1-23603f2cd3fd-2021-04-07 05:27:28Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"kT1zy+aiPWyVkB+wDtIXQmANtXD0wKPQnSajIANwvvM=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11997"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "3e1c1c67-8e00-4641-b7c1-f2f18b8617dc 4/7/2021 5:27:28 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "3e1c1c67-8e00-4641-b7c1-f2f18b8617dc"
        ],
        "x-ms-correlation-request-id": [
          "08bb7da4-11a9-4510-a623-ddab3d596f8e"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052728Z:08bb7da4-11a9-4510-a623-ddab3d596f8e"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:28 GMT"
        ],
        "Content-Length": [
          "46663"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:47.661058Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:51:55Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T17:51:55Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T16:51:37.4950021Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:35:44.8280307Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:28:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:28:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:18:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:18:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:44:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T10:44:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:34:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T10:34:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:23:10.229837Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0434\",\r\n        \"description\": \"VM reboot\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Server has come up at 20210405105909.500000-420 after a reboot or shutdown\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90081\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Server 'V2A-W2K19-202' has come up after a reboot or shutdown.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0435\",\r\n        \"description\": \"Agent upgrade\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Agent upgraded to v9.41.5888.1\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90082\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Mobility service upgraded to version '9.41.5888.1' on server 'V2A-W2K19-202'.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.5597108Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:01:33Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:01:33Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:51:32Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K19-202:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:51:32Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T05:11:04.8858693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:57:15Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T04:57:15Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:47:14Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:47:14Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:46:14.4606026Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:18:49Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T02:18:49Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"e1a52cf6-cdc5-4a8c-913b-0447da3b024d\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T01:56:50Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T01:56:50Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090755578814775807_c81ebebe-b9d1-4401-821d-5610f0a30bc9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090755578814775807_c81ebebe-b9d1-4401-821d-5610f0a30bc9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"5169c78a-7f64-430d-acbd-0318cb50b565\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-03-31T06:30:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-03-31T06:30:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=EventType%20eq%20'VmHealth'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPUV2ZW50VHlwZSUyMGVxJTIwJ1ZtSGVhbHRoJyZhcGktdmVyc2lvbj0yMDIxLTAyLTEw",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "152c71cb-e5ad-4819-9f82-c852dfde32b0"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769648644)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374448644)\\/\",\"ClientRequestId\":\"93314ae7-b033-4d89-967e-77af77b5a3c8-2021-04-07 05:27:28Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"dAFRYOlVdi8yDLTWZg6HXJWmCA6zPs2kwg9t+hg93FI=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11996"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "152c71cb-e5ad-4819-9f82-c852dfde32b0 4/7/2021 5:27:28 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "152c71cb-e5ad-4819-9f82-c852dfde32b0"
        ],
        "x-ms-correlation-request-id": [
          "84b04d12-3fde-41d5-8138-26284feb21f4"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052728Z:84b04d12-3fde-41d5-8138-26284feb21f4"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:28 GMT"
        ],
        "Content-Length": [
          "91131"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:47.661058Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.6243468Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.6243468Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.5843657Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.5843657Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T16:51:37.4950021Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7862644Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7862644Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_PositiveSourceClockSkewExceeded\",\r\n        \"description\": \"Operating system clock skewed.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7512693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7512693Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:35:44.8280307Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.9209352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.9209352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.8859347Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.8859347Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:23:10.229837Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:07:07.5003948Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T08:07:07.5003948Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7388078Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          },\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78222\",\r\n            \"summaryMessage\": \"Resynchronization required\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of following disk(s) to Azure - 'Disk1'.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7138109Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7138109Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"TargetAgentInternalError\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:40:31Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"Resync Required\",\r\n          \"component\": \"MT\",\r\n          \"correctiveAction\": \"TargetAgentInternalError\",\r\n          \"details\": \"TargetAgentInternalError\",\r\n          \"summary\": \"TargetAgentInternalError{5D628CF2-6CAA-468D-9E7A-35881C0D00DE}\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90099\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of disk ('Disk1') to Azure.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:40:31Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.5597108Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0722095Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0722095Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0422138Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0422138Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T05:11:04.8858693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3816352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3816352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3516376Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3516376Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:46:14.4606026Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.5197938Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.5197938Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.489793Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.489793Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4664319Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4664319Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4114289Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4114289Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=EventType%20eq%20'VmHealth'%20and%20AffectedObjectFriendlyName%20eq%20'V2A-W2K19-202'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPUV2ZW50VHlwZSUyMGVxJTIwJ1ZtSGVhbHRoJyUyMGFuZCUyMEFmZmVjdGVkT2JqZWN0RnJpZW5kbHlOYW1lJTIwZXElMjAnVjJBLVcySzE5LTIwMicmYXBpLXZlcnNpb249MjAyMS0wMi0xMA==",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "0674270a-db6e-41cf-a474-10b20459a71f"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769649226)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374449226)\\/\",\"ClientRequestId\":\"5f4545a7-a87c-4980-8661-d41f309d89ac-2021-04-07 05:27:29Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"2CPxc2Q1h3g3J7H47vflgd9YLxO2Ad6HIflaSQivI/M=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11995"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "0674270a-db6e-41cf-a474-10b20459a71f 4/7/2021 5:27:29 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "0674270a-db6e-41cf-a474-10b20459a71f"
        ],
        "x-ms-correlation-request-id": [
          "54f52742-cfd1-4c3d-b6bd-3cad89d177bc"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052729Z:54f52742-cfd1-4c3d-b6bd-3cad89d177bc"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:28 GMT"
        ],
        "Content-Length": [
          "24111"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=EventType%20eq%20'VmHealth'%20and%20FabricName%20eq%20'9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPUV2ZW50VHlwZSUyMGVxJTIwJ1ZtSGVhbHRoJyUyMGFuZCUyMEZhYnJpY05hbWUlMjBlcSUyMCc5YTYwZDI4YjY1NTQzNDM1ZTUyZTFiODEwNzNjNmEzYWNhN2RkNWQwMGQyMDFlNDlmYmU0ZTg2ODQ4YzQ5NWVkJyZhcGktdmVyc2lvbj0yMDIxLTAyLTEw",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "7572cc8c-d599-43bd-96d0-33b2d73eb385"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769649581)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374449581)\\/\",\"ClientRequestId\":\"6f30c745-07d1-4f78-9d67-8f7b160979aa-2021-04-07 05:27:29Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"bZK9PsJHMzTpUH7lQLcd8XWrAts5quApbTw4Rh9yF+o=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11994"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "7572cc8c-d599-43bd-96d0-33b2d73eb385 4/7/2021 5:27:29 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "7572cc8c-d599-43bd-96d0-33b2d73eb385"
        ],
        "x-ms-correlation-request-id": [
          "1492c585-62de-480e-8bcb-7f5dcf8de75b"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052729Z:1492c585-62de-480e-8bcb-7f5dcf8de75b"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:29 GMT"
        ],
        "Content-Length": [
          "91131"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:47.661058Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.6243468Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.6243468Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.5843657Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.5843657Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T16:51:37.4950021Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7862644Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7862644Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_PositiveSourceClockSkewExceeded\",\r\n        \"description\": \"Operating system clock skewed.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7512693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7512693Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:35:44.8280307Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.9209352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.9209352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.8859347Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.8859347Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:23:10.229837Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:07:07.5003948Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T08:07:07.5003948Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7388078Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          },\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78222\",\r\n            \"summaryMessage\": \"Resynchronization required\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of following disk(s) to Azure - 'Disk1'.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7138109Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7138109Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"TargetAgentInternalError\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:40:31Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"Resync Required\",\r\n          \"component\": \"MT\",\r\n          \"correctiveAction\": \"TargetAgentInternalError\",\r\n          \"details\": \"TargetAgentInternalError\",\r\n          \"summary\": \"TargetAgentInternalError{5D628CF2-6CAA-468D-9E7A-35881C0D00DE}\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90099\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of disk ('Disk1') to Azure.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:40:31Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.5597108Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0722095Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0722095Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0422138Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0422138Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T05:11:04.8858693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3816352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3816352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3516376Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3516376Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:46:14.4606026Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.5197938Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.5197938Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.489793Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.489793Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4664319Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4664319Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4114289Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4114289Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics?api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkZhYnJpY3M/YXBpLXZlcnNpb249MjAyMS0wMi0xMA==",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "b1fb02b1-f92f-4796-aa1c-1a3d637355ea"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769650006)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374450006)\\/\",\"ClientRequestId\":\"3bd5b682-ddda-4f4c-b30d-3e86da5c8846-2021-04-07 05:27:30Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"OQrs+xB4mJ+48DTxiKp58GnKeZCBmTh6v2EjPnJ48bg=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11993"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "b1fb02b1-f92f-4796-aa1c-1a3d637355ea 4/7/2021 5:27:30 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "b1fb02b1-f92f-4796-aa1c-1a3d637355ea"
        ],
        "x-ms-correlation-request-id": [
          "bc7f310c-b02c-4652-9467-24d88cced5c9"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052730Z:bc7f310c-b02c-4652-9467-24d88cced5c9"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:30 GMT"
        ],
        "Content-Length": [
          "6342"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationFabrics\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n      \"properties\": {\r\n        \"friendlyName\": \"PwsTestCS\",\r\n        \"encryptionDetails\": {\r\n          \"kekState\": \"None\",\r\n          \"kekCertThumbprint\": null\r\n        },\r\n        \"rolloverEncryptionDetails\": {\r\n          \"kekState\": \"None\",\r\n          \"kekCertThumbprint\": null\r\n        },\r\n        \"internalIdentifier\": \"01fbe5d9-6391-45fc-a729-bef0b458c9fe\",\r\n        \"bcdrState\": \"Valid\",\r\n        \"customDetails\": {\r\n          \"instanceType\": \"VMware\",\r\n          \"processServers\": [\r\n            {\r\n              \"friendlyName\": \"V2A-PS-200\",\r\n              \"id\": \"30AE6187-5869-8E44-B8AF6AA66E3ECC8F\",\r\n              \"ipAddress\": \"10.150.108.21\",\r\n              \"osType\": \"Windows\",\r\n              \"agentVersion\": \"9.41.0.0\",\r\n              \"lastHeartbeat\": \"2021-04-07T05:11:02Z\",\r\n              \"versionStatus\": \"Supported\",\r\n              \"mobilityServiceUpdates\": [\r\n                {\r\n                  \"version\": \"9.41.0.0\",\r\n                  \"rebootStatus\": \"NotRequired\",\r\n                  \"osType\": \"Windows\"\r\n                },\r\n                {\r\n                  \"version\": \"9.41.0.0\",\r\n                  \"rebootStatus\": \"NotRequired\",\r\n                  \"osType\": \"Linux\"\r\n                }\r\n              ],\r\n              \"hostId\": \"30AE6187-5869-8E44-B8AF6AA66E3ECC8F\",\r\n              \"machineCount\": \"2\",\r\n              \"replicationPairCount\": \"3\",\r\n              \"systemLoad\": \"0\",\r\n              \"systemLoadStatus\": \"Green\",\r\n              \"cpuLoad\": \"0%\",\r\n              \"cpuLoadStatus\": \"Green\",\r\n              \"totalMemoryInBytes\": 5032087552,\r\n              \"availableMemoryInBytes\": 2913578693,\r\n              \"memoryUsageStatus\": \"Green\",\r\n              \"totalSpaceInBytes\": 107337478144,\r\n              \"availableSpaceInBytes\": 104847466837,\r\n              \"spaceUsageStatus\": \"Green\",\r\n              \"psServiceStatus\": \"Running\",\r\n              \"sslCertExpiryDate\": \"2024-03-22T14:18:58Z\",\r\n              \"sslCertExpiryRemainingDays\": 1080,\r\n              \"osVersion\": null,\r\n              \"healthErrors\": [],\r\n              \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n              \"agentVersionDetails\": {\r\n                \"version\": \"9.41.0.0\",\r\n                \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n                \"status\": \"Supported\"\r\n              },\r\n              \"health\": \"Normal\",\r\n              \"historicHealth\": \"Normal\",\r\n              \"psStatsRefreshTime\": \"2021-04-07T05:11:50Z\",\r\n              \"throughputUploadPendingDataInBytes\": 0,\r\n              \"throughputInMBps\": 0,\r\n              \"throughputInBytes\": 9876,\r\n              \"throughputStatus\": \"Green\",\r\n              \"marsCommunicationStatus\": \"1\",\r\n              \"marsRegistrationStatus\": \"1\"\r\n            },\r\n            {\r\n              \"friendlyName\": \"PwsTestCS\",\r\n              \"id\": \"14968BDF-E18E-7F45-A9B0E3EFAC2DDCA2\",\r\n              \"ipAddress\": \"10.144.130.132\",\r\n              \"osType\": \"Windows\",\r\n              \"agentVersion\": \"9.41.0.0\",\r\n              \"lastHeartbeat\": \"2021-04-07T05:11:19Z\",\r\n              \"versionStatus\": \"Supported\",\r\n              \"mobilityServiceUpdates\": [\r\n                {\r\n                  \"version\": \"9.41.0.0\",\r\n                  \"rebootStatus\": \"NotRequired\",\r\n                  \"osType\": \"Windows\"\r\n                },\r\n                {\r\n                  \"version\": \"9.41.0.0\",\r\n                  \"rebootStatus\": \"NotRequired\",\r\n                  \"osType\": \"Linux\"\r\n                }\r\n              ],\r\n              \"hostId\": \"14968BDF-E18E-7F45-A9B0E3EFAC2DDCA2\",\r\n              \"machineCount\": \"0\",\r\n              \"replicationPairCount\": \"0\",\r\n              \"systemLoad\": \"0\",\r\n              \"systemLoadStatus\": \"Green\",\r\n              \"cpuLoad\": \"0%\",\r\n              \"cpuLoadStatus\": \"Green\",\r\n              \"totalMemoryInBytes\": 20266405888,\r\n              \"availableMemoryInBytes\": 15846302764,\r\n              \"memoryUsageStatus\": \"Green\",\r\n              \"totalSpaceInBytes\": 137435803648,\r\n              \"availableSpaceInBytes\": 133164775765,\r\n              \"spaceUsageStatus\": \"Green\",\r\n              \"psServiceStatus\": \"Running\",\r\n              \"sslCertExpiryDate\": \"2024-03-18T14:07:04Z\",\r\n              \"sslCertExpiryRemainingDays\": 1076,\r\n              \"osVersion\": null,\r\n              \"healthErrors\": [],\r\n              \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n              \"agentVersionDetails\": {\r\n                \"version\": \"9.41.0.0\",\r\n                \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n                \"status\": \"Supported\"\r\n              },\r\n              \"health\": \"Normal\",\r\n              \"historicHealth\": \"Normal\",\r\n              \"psStatsRefreshTime\": \"2021-04-07T05:11:51Z\",\r\n              \"throughputUploadPendingDataInBytes\": 0,\r\n              \"throughputInMBps\": 0,\r\n              \"throughputInBytes\": 0,\r\n              \"throughputStatus\": \"Green\",\r\n              \"marsCommunicationStatus\": \"1\",\r\n              \"marsRegistrationStatus\": \"1\"\r\n            }\r\n          ],\r\n          \"masterTargetServers\": [\r\n            {\r\n              \"id\": \"14968BDF-E18E-7F45-A9B0E3EFAC2DDCA2\",\r\n              \"ipAddress\": \"10.144.130.132\",\r\n              \"name\": \"PwsTestCS\",\r\n              \"osType\": \"Windows\",\r\n              \"agentVersion\": \"9.41.0.0\",\r\n              \"lastHeartbeat\": \"2021-04-07T05:10:54Z\",\r\n              \"versionStatus\": \"Supported\",\r\n              \"retentionVolumes\": [\r\n                {\r\n                  \"volumeName\": \"D\",\r\n                  \"capacityInBytes\": 34357637120,\r\n                  \"freeSpaceInBytes\": 31186448384,\r\n                  \"thresholdPercentage\": null\r\n                }\r\n              ],\r\n              \"dataStores\": [],\r\n              \"validationErrors\": [],\r\n              \"healthErrors\": [],\r\n              \"diskCount\": 0,\r\n              \"osVersion\": \"Microsoft Windows Server 2016 Datacenter\",\r\n              \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n              \"marsAgentVersion\": \"2.0.9202.0\",\r\n              \"marsAgentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n              \"agentVersionDetails\": {\r\n                \"version\": \"9.41.0.0\",\r\n                \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n                \"status\": \"Supported\"\r\n              },\r\n              \"marsAgentVersionDetails\": {\r\n                \"version\": \"2.0.9202.0\",\r\n                \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n                \"status\": \"Supported\"\r\n              }\r\n            },\r\n            {\r\n              \"id\": \"30AE6187-5869-8E44-B8AF6AA66E3ECC8F\",\r\n              \"ipAddress\": \"10.150.108.21\",\r\n              \"name\": \"V2A-PS-200\",\r\n              \"osType\": \"Windows\",\r\n              \"agentVersion\": \"9.41.0.0\",\r\n              \"lastHeartbeat\": \"2021-04-07T05:10:53Z\",\r\n              \"versionStatus\": \"Supported\",\r\n              \"retentionVolumes\": [\r\n                {\r\n                  \"volumeName\": \"F\",\r\n                  \"capacityInBytes\": 10734268416,\r\n                  \"freeSpaceInBytes\": 10688544768,\r\n                  \"thresholdPercentage\": null\r\n                }\r\n              ],\r\n              \"dataStores\": [\r\n                {\r\n                  \"symbolicName\": \"datastore1\",\r\n                  \"uuid\": \"6005a082-e4a06cce-c50c-f0d4e2e6578e\",\r\n                  \"capacity\": \"439.5\",\r\n                  \"freeSpace\": \"154.11\",\r\n                  \"type\": \"VMFS\"\r\n                },\r\n                {\r\n                  \"symbolicName\": \"DS-77\",\r\n                  \"uuid\": \"5c5d35b2-509cd314-afbf-f8bc124d5eea\",\r\n                  \"capacity\": \"2047.75\",\r\n                  \"freeSpace\": \"242.3\",\r\n                  \"type\": \"VMFS\"\r\n                }\r\n              ],\r\n              \"validationErrors\": [],\r\n              \"healthErrors\": [],\r\n              \"diskCount\": 3,\r\n              \"osVersion\": \"Microsoft Windows Server 2016 Datacenter\",\r\n              \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n              \"marsAgentVersion\": \"2.0.9202.0\",\r\n              \"marsAgentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n              \"agentVersionDetails\": {\r\n                \"version\": \"9.41.0.0\",\r\n                \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n                \"status\": \"Supported\"\r\n              },\r\n              \"marsAgentVersionDetails\": {\r\n                \"version\": \"2.0.9202.0\",\r\n                \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n                \"status\": \"Supported\"\r\n              }\r\n            }\r\n          ],\r\n          \"runAsAccounts\": [\r\n            {\r\n              \"accountId\": \"1\",\r\n              \"accountName\": \"vcenteruser\"\r\n            },\r\n            {\r\n              \"accountId\": \"2\",\r\n              \"accountName\": \"windowsuser\"\r\n            },\r\n            {\r\n              \"accountId\": \"3\",\r\n              \"accountName\": \"linuxuser\"\r\n            }\r\n          ],\r\n          \"replicationPairCount\": \"3\",\r\n          \"processServerCount\": \"2\",\r\n          \"agentCount\": \"3\",\r\n          \"protectedServers\": \"2\",\r\n          \"systemLoad\": \"0\",\r\n          \"systemLoadStatus\": \"Green\",\r\n          \"cpuLoad\": \"0%\",\r\n          \"cpuLoadStatus\": \"Green\",\r\n          \"totalMemoryInBytes\": 20266405888,\r\n          \"availableMemoryInBytes\": 15846302764,\r\n          \"memoryUsageStatus\": \"Green\",\r\n          \"totalSpaceInBytes\": 137435803648,\r\n          \"availableSpaceInBytes\": 133164775765,\r\n          \"spaceUsageStatus\": \"Green\",\r\n          \"webLoad\": \"0\",\r\n          \"webLoadStatus\": \"Green\",\r\n          \"databaseServerLoad\": \"0.25\",\r\n          \"databaseServerLoadStatus\": \"Green\",\r\n          \"csServiceStatus\": \"Running\",\r\n          \"ipAddress\": \"10.144.130.132\",\r\n          \"agentVersion\": \"9.41.0.0\",\r\n          \"hostName\": \"PwsTestCS\",\r\n          \"lastHeartbeat\": \"2021-04-07T05:12:20.1334445Z\",\r\n          \"versionStatus\": \"Supported\",\r\n          \"sslCertExpiryDate\": \"2024-03-18T14:07:03Z\",\r\n          \"sslCertExpiryRemainingDays\": 1076,\r\n          \"psTemplateVersion\": \"202102.14.00\",\r\n          \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"agentVersionDetails\": {\r\n            \"version\": \"9.41.0.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          }\r\n        },\r\n        \"healthErrorDetails\": [],\r\n        \"health\": \"Normal\"\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed?api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkZhYnJpY3MvOWE2MGQyOGI2NTU0MzQzNWU1MmUxYjgxMDczYzZhM2FjYTdkZDVkMDBkMjAxZTQ5ZmJlNGU4Njg0OGM0OTVlZD9hcGktdmVyc2lvbj0yMDIxLTAyLTEw",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "b1fb02b1-f92f-4796-aa1c-1a3d637355ea"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769650885)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374450885)\\/\",\"ClientRequestId\":\"666eb177-81fa-448d-966e-94b75a388f1a-2021-04-07 05:27:30Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"+u4a8BXQKmcIzzruF4PsBThNEpkz4Xu76XgQKdOROa4=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11992"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "b1fb02b1-f92f-4796-aa1c-1a3d637355ea 4/7/2021 5:27:31 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "b1fb02b1-f92f-4796-aa1c-1a3d637355ea"
        ],
        "x-ms-correlation-request-id": [
          "bfd968f0-52c5-494d-8969-11366593cfb1"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052731Z:bfd968f0-52c5-494d-8969-11366593cfb1"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:30 GMT"
        ],
        "Content-Length": [
          "6262"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"name\": \"9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n  \"type\": \"Microsoft.RecoveryServices/vaults/replicationFabrics\",\r\n  \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n  \"properties\": {\r\n    \"friendlyName\": \"PwsTestCS\",\r\n    \"encryptionDetails\": {\r\n      \"kekState\": \"None\",\r\n      \"kekCertThumbprint\": null\r\n    },\r\n    \"rolloverEncryptionDetails\": {\r\n      \"kekState\": \"None\",\r\n      \"kekCertThumbprint\": null\r\n    },\r\n    \"internalIdentifier\": \"01fbe5d9-6391-45fc-a729-bef0b458c9fe\",\r\n    \"bcdrState\": \"Valid\",\r\n    \"customDetails\": {\r\n      \"instanceType\": \"VMware\",\r\n      \"processServers\": [\r\n        {\r\n          \"friendlyName\": \"V2A-PS-200\",\r\n          \"id\": \"30AE6187-5869-8E44-B8AF6AA66E3ECC8F\",\r\n          \"ipAddress\": \"10.150.108.21\",\r\n          \"osType\": \"Windows\",\r\n          \"agentVersion\": \"9.41.0.0\",\r\n          \"lastHeartbeat\": \"2021-04-07T05:11:02Z\",\r\n          \"versionStatus\": \"Supported\",\r\n          \"mobilityServiceUpdates\": [\r\n            {\r\n              \"version\": \"9.41.0.0\",\r\n              \"rebootStatus\": \"NotRequired\",\r\n              \"osType\": \"Windows\"\r\n            },\r\n            {\r\n              \"version\": \"9.41.0.0\",\r\n              \"rebootStatus\": \"NotRequired\",\r\n              \"osType\": \"Linux\"\r\n            }\r\n          ],\r\n          \"hostId\": \"30AE6187-5869-8E44-B8AF6AA66E3ECC8F\",\r\n          \"machineCount\": \"2\",\r\n          \"replicationPairCount\": \"3\",\r\n          \"systemLoad\": \"0\",\r\n          \"systemLoadStatus\": \"Green\",\r\n          \"cpuLoad\": \"0%\",\r\n          \"cpuLoadStatus\": \"Green\",\r\n          \"totalMemoryInBytes\": 5032087552,\r\n          \"availableMemoryInBytes\": 2913578693,\r\n          \"memoryUsageStatus\": \"Green\",\r\n          \"totalSpaceInBytes\": 107337478144,\r\n          \"availableSpaceInBytes\": 104847466837,\r\n          \"spaceUsageStatus\": \"Green\",\r\n          \"psServiceStatus\": \"Running\",\r\n          \"sslCertExpiryDate\": \"2024-03-22T14:18:58Z\",\r\n          \"sslCertExpiryRemainingDays\": 1080,\r\n          \"osVersion\": null,\r\n          \"healthErrors\": [],\r\n          \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"agentVersionDetails\": {\r\n            \"version\": \"9.41.0.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          },\r\n          \"health\": \"Normal\",\r\n          \"psStatsRefreshTime\": \"2021-04-07T05:11:50Z\",\r\n          \"throughputUploadPendingDataInBytes\": 0,\r\n          \"throughputInMBps\": 0,\r\n          \"throughputInBytes\": 9876,\r\n          \"throughputStatus\": \"Green\",\r\n          \"marsCommunicationStatus\": \"1\",\r\n          \"marsRegistrationStatus\": \"1\"\r\n        },\r\n        {\r\n          \"friendlyName\": \"PwsTestCS\",\r\n          \"id\": \"14968BDF-E18E-7F45-A9B0E3EFAC2DDCA2\",\r\n          \"ipAddress\": \"10.144.130.132\",\r\n          \"osType\": \"Windows\",\r\n          \"agentVersion\": \"9.41.0.0\",\r\n          \"lastHeartbeat\": \"2021-04-07T05:11:19Z\",\r\n          \"versionStatus\": \"Supported\",\r\n          \"mobilityServiceUpdates\": [\r\n            {\r\n              \"version\": \"9.41.0.0\",\r\n              \"rebootStatus\": \"NotRequired\",\r\n              \"osType\": \"Windows\"\r\n            },\r\n            {\r\n              \"version\": \"9.41.0.0\",\r\n              \"rebootStatus\": \"NotRequired\",\r\n              \"osType\": \"Linux\"\r\n            }\r\n          ],\r\n          \"hostId\": \"14968BDF-E18E-7F45-A9B0E3EFAC2DDCA2\",\r\n          \"machineCount\": \"0\",\r\n          \"replicationPairCount\": \"0\",\r\n          \"systemLoad\": \"0\",\r\n          \"systemLoadStatus\": \"Green\",\r\n          \"cpuLoad\": \"0%\",\r\n          \"cpuLoadStatus\": \"Green\",\r\n          \"totalMemoryInBytes\": 20266405888,\r\n          \"availableMemoryInBytes\": 15846302764,\r\n          \"memoryUsageStatus\": \"Green\",\r\n          \"totalSpaceInBytes\": 137435803648,\r\n          \"availableSpaceInBytes\": 133164775765,\r\n          \"spaceUsageStatus\": \"Green\",\r\n          \"psServiceStatus\": \"Running\",\r\n          \"sslCertExpiryDate\": \"2024-03-18T14:07:04Z\",\r\n          \"sslCertExpiryRemainingDays\": 1076,\r\n          \"osVersion\": null,\r\n          \"healthErrors\": [],\r\n          \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"agentVersionDetails\": {\r\n            \"version\": \"9.41.0.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          },\r\n          \"health\": \"Normal\",\r\n          \"psStatsRefreshTime\": \"2021-04-07T05:11:51Z\",\r\n          \"throughputUploadPendingDataInBytes\": 0,\r\n          \"throughputInMBps\": 0,\r\n          \"throughputInBytes\": 0,\r\n          \"throughputStatus\": \"Green\",\r\n          \"marsCommunicationStatus\": \"1\",\r\n          \"marsRegistrationStatus\": \"1\"\r\n        }\r\n      ],\r\n      \"masterTargetServers\": [\r\n        {\r\n          \"id\": \"14968BDF-E18E-7F45-A9B0E3EFAC2DDCA2\",\r\n          \"ipAddress\": \"10.144.130.132\",\r\n          \"name\": \"PwsTestCS\",\r\n          \"osType\": \"Windows\",\r\n          \"agentVersion\": \"9.41.0.0\",\r\n          \"lastHeartbeat\": \"2021-04-07T05:10:54Z\",\r\n          \"versionStatus\": \"Supported\",\r\n          \"retentionVolumes\": [\r\n            {\r\n              \"volumeName\": \"D\",\r\n              \"capacityInBytes\": 34357637120,\r\n              \"freeSpaceInBytes\": 31186448384,\r\n              \"thresholdPercentage\": null\r\n            }\r\n          ],\r\n          \"dataStores\": [],\r\n          \"validationErrors\": [],\r\n          \"healthErrors\": [],\r\n          \"diskCount\": 0,\r\n          \"osVersion\": \"Microsoft Windows Server 2016 Datacenter\",\r\n          \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"marsAgentVersion\": \"2.0.9202.0\",\r\n          \"marsAgentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"agentVersionDetails\": {\r\n            \"version\": \"9.41.0.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          },\r\n          \"marsAgentVersionDetails\": {\r\n            \"version\": \"2.0.9202.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          }\r\n        },\r\n        {\r\n          \"id\": \"30AE6187-5869-8E44-B8AF6AA66E3ECC8F\",\r\n          \"ipAddress\": \"10.150.108.21\",\r\n          \"name\": \"V2A-PS-200\",\r\n          \"osType\": \"Windows\",\r\n          \"agentVersion\": \"9.41.0.0\",\r\n          \"lastHeartbeat\": \"2021-04-07T05:10:53Z\",\r\n          \"versionStatus\": \"Supported\",\r\n          \"retentionVolumes\": [\r\n            {\r\n              \"volumeName\": \"F\",\r\n              \"capacityInBytes\": 10734268416,\r\n              \"freeSpaceInBytes\": 10688544768,\r\n              \"thresholdPercentage\": null\r\n            }\r\n          ],\r\n          \"dataStores\": [\r\n            {\r\n              \"symbolicName\": \"datastore1\",\r\n              \"uuid\": \"6005a082-e4a06cce-c50c-f0d4e2e6578e\",\r\n              \"capacity\": \"439.5\",\r\n              \"freeSpace\": \"154.11\",\r\n              \"type\": \"VMFS\"\r\n            },\r\n            {\r\n              \"symbolicName\": \"DS-77\",\r\n              \"uuid\": \"5c5d35b2-509cd314-afbf-f8bc124d5eea\",\r\n              \"capacity\": \"2047.75\",\r\n              \"freeSpace\": \"242.3\",\r\n              \"type\": \"VMFS\"\r\n            }\r\n          ],\r\n          \"validationErrors\": [],\r\n          \"healthErrors\": [],\r\n          \"diskCount\": 3,\r\n          \"osVersion\": \"Microsoft Windows Server 2016 Datacenter\",\r\n          \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"marsAgentVersion\": \"2.0.9202.0\",\r\n          \"marsAgentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n          \"agentVersionDetails\": {\r\n            \"version\": \"9.41.0.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          },\r\n          \"marsAgentVersionDetails\": {\r\n            \"version\": \"2.0.9202.0\",\r\n            \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n            \"status\": \"Supported\"\r\n          }\r\n        }\r\n      ],\r\n      \"runAsAccounts\": [\r\n        {\r\n          \"accountId\": \"1\",\r\n          \"accountName\": \"vcenteruser\"\r\n        },\r\n        {\r\n          \"accountId\": \"2\",\r\n          \"accountName\": \"windowsuser\"\r\n        },\r\n        {\r\n          \"accountId\": \"3\",\r\n          \"accountName\": \"linuxuser\"\r\n        }\r\n      ],\r\n      \"replicationPairCount\": \"3\",\r\n      \"processServerCount\": \"2\",\r\n      \"agentCount\": \"3\",\r\n      \"protectedServers\": \"2\",\r\n      \"systemLoad\": \"0\",\r\n      \"systemLoadStatus\": \"Green\",\r\n      \"cpuLoad\": \"0%\",\r\n      \"cpuLoadStatus\": \"Green\",\r\n      \"totalMemoryInBytes\": 20266405888,\r\n      \"availableMemoryInBytes\": 15846302764,\r\n      \"memoryUsageStatus\": \"Green\",\r\n      \"totalSpaceInBytes\": 137435803648,\r\n      \"availableSpaceInBytes\": 133164775765,\r\n      \"spaceUsageStatus\": \"Green\",\r\n      \"webLoad\": \"0\",\r\n      \"webLoadStatus\": \"Green\",\r\n      \"databaseServerLoad\": \"0.25\",\r\n      \"databaseServerLoadStatus\": \"Green\",\r\n      \"csServiceStatus\": \"Running\",\r\n      \"ipAddress\": \"10.144.130.132\",\r\n      \"agentVersion\": \"9.41.0.0\",\r\n      \"hostName\": \"PwsTestCS\",\r\n      \"lastHeartbeat\": \"2021-04-07T05:12:20.1334445Z\",\r\n      \"versionStatus\": \"Supported\",\r\n      \"sslCertExpiryDate\": \"2024-03-18T14:07:03Z\",\r\n      \"sslCertExpiryRemainingDays\": 1076,\r\n      \"psTemplateVersion\": \"202102.14.00\",\r\n      \"agentExpiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n      \"agentVersionDetails\": {\r\n        \"version\": \"9.41.0.0\",\r\n        \"expiryDate\": \"9999-12-31T23:59:59.9999999\",\r\n        \"status\": \"Supported\"\r\n      }\r\n    },\r\n    \"healthErrorDetails\": [],\r\n    \"health\": \"Normal\"\r\n  }\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=FabricName%20eq%20'9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPUZhYnJpY05hbWUlMjBlcSUyMCc5YTYwZDI4YjY1NTQzNDM1ZTUyZTFiODEwNzNjNmEzYWNhN2RkNWQwMGQyMDFlNDlmYmU0ZTg2ODQ4YzQ5NWVkJyZhcGktdmVyc2lvbj0yMDIxLTAyLTEw",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "e2816e0a-7ceb-4ed7-bf15-2a561aac9bf0"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769651410)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374451410)\\/\",\"ClientRequestId\":\"deb55d2b-f5d8-482a-928a-fbf0b5d8d63b-2021-04-07 05:27:31Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"/xfbJ1eVxNh/vgeHl9DzYiCQazVsQ5jc5JMF9gWxX00=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11991"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "e2816e0a-7ceb-4ed7-bf15-2a561aac9bf0 4/7/2021 5:27:31 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "e2816e0a-7ceb-4ed7-bf15-2a561aac9bf0"
        ],
        "x-ms-correlation-request-id": [
          "307a2779-77a2-4ed8-841c-6d9ebcac830b"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052731Z:307a2779-77a2-4ed8-841c-6d9ebcac830b"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:30 GMT"
        ],
        "Content-Length": [
          "126319"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:47.661058Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:51:55Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T17:51:55Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.6243468Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.6243468Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.5843657Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.5843657Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T16:51:37.4950021Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7862644Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7862644Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_PositiveSourceClockSkewExceeded\",\r\n        \"description\": \"Operating system clock skewed.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7512693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7512693Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:35:44.8280307Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:28:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:28:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.9209352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.9209352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.8859347Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.8859347Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:18:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:18:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:44:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T10:44:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:34:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T10:34:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:23:10.229837Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:07:07.5003948Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T08:07:07.5003948Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7388078Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          },\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78222\",\r\n            \"summaryMessage\": \"Resynchronization required\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of following disk(s) to Azure - 'Disk1'.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7138109Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7138109Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0434\",\r\n        \"description\": \"VM reboot\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Server has come up at 20210405105909.500000-420 after a reboot or shutdown\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90081\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Server 'V2A-W2K19-202' has come up after a reboot or shutdown.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0435\",\r\n        \"description\": \"Agent upgrade\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Agent upgraded to v9.41.5888.1\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90082\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Mobility service upgraded to version '9.41.5888.1' on server 'V2A-W2K19-202'.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"TargetAgentInternalError\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:40:31Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"Resync Required\",\r\n          \"component\": \"MT\",\r\n          \"correctiveAction\": \"TargetAgentInternalError\",\r\n          \"details\": \"TargetAgentInternalError\",\r\n          \"summary\": \"TargetAgentInternalError{5D628CF2-6CAA-468D-9E7A-35881C0D00DE}\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90099\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of disk ('Disk1') to Azure.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:40:31Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.5597108Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0722095Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0722095Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0422138Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0422138Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:01:33Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:01:33Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:51:32Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K19-202:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:51:32Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T05:11:04.8858693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:57:15Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T04:57:15Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3816352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3816352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3516376Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3516376Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:47:14Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:47:14Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:46:14.4606026Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.5197938Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.5197938Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.489793Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.489793Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:18:49Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T02:18:49Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4664319Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4664319Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4114289Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4114289Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"e1a52cf6-cdc5-4a8c-913b-0447da3b024d\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T01:56:50Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T01:56:50Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090755578814775807_c81ebebe-b9d1-4401-821d-5610f0a30bc9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090755578814775807_c81ebebe-b9d1-4401-821d-5610f0a30bc9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"5169c78a-7f64-430d-acbd-0318cb50b565\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-03-31T06:30:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-03-31T06:30:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=AffectedObjectFriendlyName%20eq%20'V2A-W2K19-202'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPUFmZmVjdGVkT2JqZWN0RnJpZW5kbHlOYW1lJTIwZXElMjAnVjJBLVcySzE5LTIwMicmYXBpLXZlcnNpb249MjAyMS0wMi0xMA==",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "d87b2a82-c5ed-4720-9e0f-ec49f4dbd5e7"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769651853)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374451853)\\/\",\"ClientRequestId\":\"bfb82f24-17bd-41ed-b303-5113582ca156-2021-04-07 05:27:31Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"aG1E2Iqd5d5VwG6OTJYeCqGJZS1zep7XZhQi/mVBbKc=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11990"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "d87b2a82-c5ed-4720-9e0f-ec49f4dbd5e7 4/7/2021 5:27:32 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "d87b2a82-c5ed-4720-9e0f-ec49f4dbd5e7"
        ],
        "x-ms-correlation-request-id": [
          "1d0f9496-547f-4505-8151-8c7337217c7a"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052732Z:1d0f9496-547f-4505-8151-8c7337217c7a"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:31 GMT"
        ],
        "Content-Length": [
          "37431"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:51:55Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T17:51:55Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0434\",\r\n        \"description\": \"VM reboot\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Server has come up at 20210405105909.500000-420 after a reboot or shutdown\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90081\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Server 'V2A-W2K19-202' has come up after a reboot or shutdown.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0435\",\r\n        \"description\": \"Agent upgrade\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Agent upgraded to v9.41.5888.1\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90082\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Mobility service upgraded to version '9.41.5888.1' on server 'V2A-W2K19-202'.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:01:33Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:01:33Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:51:32Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K19-202:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:51:32Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:18:49Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T02:18:49Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    },
    {
      "RequestUri": "/subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents?$filter=StartTime%20eq%20'4/4/2021%2010:57:32%20AM'&api-version=2021-02-10",
      "EncodedRequestUri": "L3N1YnNjcmlwdGlvbnMvYjhhZWY4ZTEtMzdkZi00ZjE3LWE1MzctZjEwZTE4M2M4ZWNhL3Jlc291cmNlR3JvdXBzL1B3c1Rlc3RSRy9wcm92aWRlcnMvTWljcm9zb2Z0LlJlY292ZXJ5U2VydmljZXMvdmF1bHRzL1B3c1Rlc3RWYXVsdC9yZXBsaWNhdGlvbkV2ZW50cz8kZmlsdGVyPVN0YXJ0VGltZSUyMGVxJTIwJzQvNC8yMDIxJTIwMTA6NTc6MzIlMjBBTScmYXBpLXZlcnNpb249MjAyMS0wMi0xMA==",
      "RequestMethod": "GET",
      "RequestBody": "",
      "RequestHeaders": {
        "x-ms-client-request-id": [
          "8b354bb4-7421-485b-a2fc-2c464f782049"
        ],
        "Accept-Language": [
          "en-US"
        ],
        "Agent-Authentication": [
          "{\"NotBeforeTimestamp\":\"\\/Date(1617769652229)\\/\",\"NotAfterTimestamp\":\"\\/Date(1618374452229)\\/\",\"ClientRequestId\":\"22aea18d-4fb1-4d5c-acf5-c7849ca0ccdc-2021-04-07 05:27:32Z-Ps\",\"HashFunction\":\"HMACSHA256\",\"Hmac\":\"ZeM2vyfC/qtPaG+w/qlVTQl+ILvHSZElWGU8nJJKCko=\",\"Version\":{\"Major\":1,\"Minor\":2,\"Build\":-1,\"Revision\":-1,\"MajorRevision\":-1,\"MinorRevision\":-1},\"PropertyBag\":{}}"
        ],
        "User-Agent": [
          "FxVersion/4.6.29812.02",
          "OSName/Windows",
          "OSVersion/Microsoft.Windows.10.0.19042.",
          "Microsoft.Azure.Management.RecoveryServices.SiteRecovery.SiteRecoveryManagementClient/2.1.4.0"
        ]
      },
      "ResponseHeaders": {
        "Cache-Control": [
          "no-cache"
        ],
        "Pragma": [
          "no-cache"
        ],
        "x-ms-ratelimit-remaining-subscription-reads": [
          "11989"
        ],
        "Server": [
          "Microsoft-IIS/10.0",
          "Kestrel"
        ],
        "Strict-Transport-Security": [
          "max-age=31536000; includeSubDomains"
        ],
        "x-ms-request-id": [
          "8b354bb4-7421-485b-a2fc-2c464f782049 4/7/2021 5:27:32 AM"
        ],
        "X-Content-Type-Options": [
          "nosniff"
        ],
        "x-ms-client-request-id": [
          "8b354bb4-7421-485b-a2fc-2c464f782049"
        ],
        "x-ms-correlation-request-id": [
          "2705d058-f8a2-4d0a-be0d-2b5590e4eed8"
        ],
        "x-ms-routing-request-id": [
          "CENTRALUSEUAP:20210407T052732Z:2705d058-f8a2-4d0a-be0d-2b5590e4eed8"
        ],
        "Date": [
          "Wed, 07 Apr 2021 05:27:31 GMT"
        ],
        "Content-Length": [
          "123118"
        ],
        "Content-Type": [
          "application/json"
        ],
        "Expires": [
          "-1"
        ]
      },
      "ResponseBody": "{\r\n  \"value\": [\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980973565243_bf1321fb-e3c7-47b8-9ef1-54aada0d6cf9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:48.1210564Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749980978165227_f724e9a7-94e6-4c4e-a65f-82d50e9169bc\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:59:47.661058Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090749985704775807_3c357390-b1b3-451a-ad66-8b5d657b2269\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:51:55Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T17:51:55Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575332277_ebbec25c-3731-4f47-81d3-35c69f8d2727\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.944353Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.944353Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990575632287_c1115b4f-17e3-4252-8241-9d8fa32b6ae3\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"d0b27bd4-c677-4c71-bf96-15fa3b38dca0\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.914352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.914352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578532339_463afc76-9cb6-4d1e-84a3-c461f02c2273\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.6243468Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.6243468Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090749990578932150_550deac3-f727-49e4-baff-d63a1920b5e7\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"75eb2d62-0f62-4f56-9a28-56d2ac8abbe5\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T17:43:47.5843657Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T17:43:47.5843657Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750021879825786_209f6a50-cc73-4c70-95a4-09f2711fe7cf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T16:51:37.4950021Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067036913163_f8e1786b-92ef-4584-8a1d-bcecbadd2768\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7862644Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7862644Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067037263114_5b42c6ef-5c21-451c-9427-ac134c8b5de5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_PositiveSourceClockSkewExceeded\",\r\n        \"description\": \"Operating system clock skewed.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:36:21.7512693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78193\",\r\n            \"summaryMessage\": \"Operating system clock skewed\",\r\n            \"errorMessage\": \"The operating system clock on the machine is skewed by more than 10 minutes.\",\r\n            \"possibleCauses\": \"The time reported by the Operating system on the virtual machine is skewed positively with respect to actual time. The OS reported time on the virtual machine acts as a frame of reference for the labeling and ordering of recovery points. A significant skew in the time settings will impact the labeling of recovery points, RPO estimates, and the ability to generate recovery points for the machine.\",\r\n            \"recommendedAction\": \"Fix the system time on the virtual machine.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:36:21.7512693Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750067406495500_926baae5-665f-4e1c-8e07-d0284011eab2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:35:44.8280307Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750071644775807_e690a144-957f-477c-b93f-11c06292a441\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:28:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T15:28:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015566455_3487a36c-7a84-4414-a942-fde0b1ea1ddf\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.9209352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.9209352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750077015916460_86abefcc-423e-4a1b-82f6-e2fd0921706d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:19:43.8859347Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:19:43.8859347Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750077644775807_da9d47c5-097e-4d8b-9a7f-0acb57256be6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"7bfda5b4-2f57-426a-b6bd-0699ba97df25\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T15:18:41Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T15:18:41Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750242024775807_d188c978-475f-4862-bd8f-83566903843a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:44:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T10:44:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750248024775807_b77b3511-d9b1-4e2d-ab03-fe8a3ea21423\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"01810881-4942-453f-9b19-3e68e25c6eab\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T10:34:43Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(30.30.0.4)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T10:34:43Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750326952477437_384c7d23-647a-4362-9fce-685b0876f3d8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:23:10.229837Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750336579771859_4389b950-7a3b-42b7-981e-8eef939bd3a6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T08:07:07.5003948Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T08:07:07.5003948Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187387729_1ff9da38-3052-498a-bc4a-26b40413521e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7388078Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          },\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78222\",\r\n            \"summaryMessage\": \"Resynchronization required\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of following disk(s) to Azure - 'Disk1'.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7388078Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750346187637698_b7a3d624-9db9-4def-8358-1d5a3f83811e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:51:06.7138109Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:51:06.7138109Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_2d621a10-c48d-4d9c-bde3-1a124c804131\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0434\",\r\n        \"description\": \"VM reboot\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Server has come up at 20210405105909.500000-420 after a reboot or shutdown\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90081\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Server 'V2A-W2K19-202' has come up after a reboot or shutdown.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750351014775807_ea134334-1b3e-49ea-9e14-6f17b6abd88c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0435\",\r\n        \"description\": \"Agent upgrade\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:43:04Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"-\",\r\n          \"details\": \"Agent upgraded to v9.41.5888.1\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"On V2A-W2K19-202: ALERT Observed/Logged\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90082\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Mobility service upgraded to version '9.41.5888.1' on server 'V2A-W2K19-202'.\",\r\n            \"possibleCauses\": \"-\",\r\n            \"recommendedAction\": \"-\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:43:04Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750352544775807_018f26da-eafd-42c4-bd5f-db5fa3e1fe7c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"TargetAgentInternalError\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:40:31Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"Resync Required\",\r\n          \"component\": \"MT\",\r\n          \"correctiveAction\": \"TargetAgentInternalError\",\r\n          \"details\": \"TargetAgentInternalError\",\r\n          \"summary\": \"TargetAgentInternalError{5D628CF2-6CAA-468D-9E7A-35881C0D00DE}\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90099\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine 'V2A-W2K16-201' are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Due to an internal error, failed to replicate changes of disk ('Disk1') to Azure.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:40:31Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365386328745_cd69e8d1-7352-4155-945a-8bf661310ce1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.8447062Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750365389178699_7ae29ec2-671b-4185-8aad-7f6bf69f25ce\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:19:06.5597108Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374990803703_0f9e6b0f-a34c-47aa-8896-ab4058fe305c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3972104Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3972104Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374991203589_5ff5b464-8ce0-42c5-adc8-d8a551f67fa5\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.3572218Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.3572218Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994053712_697f7fab-fc2d-4b1f-96f5-74d48ad9d971\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0722095Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0722095Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750374994353669_0d575991-5b1c-43d1-87bb-0190dd888d25\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:03:06.0422138Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T07:03:06.0422138Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750375924775807_25f6f30d-1ed2-4a2c-bc03-7079daadf155\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T07:01:33Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T07:01:33Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_201d3fe0-38b9-4229-a8dd-a068a6417083\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750378804775807_3596d608-dc62-44a8-8c8a-ab5ef70c7ec6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"ConfigurationServerPsFailOver\",\r\n        \"description\": \"Resynchronization required.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:56:45Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"VM health\",\r\n          \"category\": \"PS Node Failover Alert\",\r\n          \"component\": \"CS\",\r\n          \"correctiveAction\": \"ConfigurationServerPsFailOver\",\r\n          \"details\": \"Process server failover occured from PwsTestCS [10.144.130.132] to V2A-PS-200 [10.150.108.21].\",\r\n          \"summary\": \"Process server Failover\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90103\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Resynchronization is required as one or more disks of the machine are inconsistent with the target replication disks in Azure\",\r\n            \"possibleCauses\": \"Machine has been moved from process server ('PwsTestCS') to another process server ('V2A-PS-200') through Load balance/Switch capability.\",\r\n            \"recommendedAction\": \"\\n      To resolve, a resynchronization of the machine is required. ASR will automatically initiate the resynchronization operation between automatic resynchronization window OR\\n      you can manually initiate the operation by navigating to VM overview blade and click on \\\"Resynchronize\\\".  Learn more (https://go.microsoft.com/fwlink/?linkid=2148920)\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:56:45Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750381934775807_70da96ed-2803-4c91-af29-4a0bd1a2e81d\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"5c109bb5-972a-4615-9f42-e82c9609e793\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T06:51:32Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K19-202:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T06:51:32Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750442205917114_c71c0e17-dbae-4e6b-b30c-7e4fe363f641\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T05:11:04.8858693Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750450504775807_7921d519-5fbf-4c8b-a9c6-d554d97cfe1e\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:57:15Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-06T04:57:15Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451750959455_4c052ce6-14a2-4c0e-aec8-9a22da2d5581\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3816352Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3816352Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090750451751259431_3f4294ee-03b5-48b5-ba94-fc5f342dc8e2\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:55:10.3516376Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:55:10.3516376Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090750456514775807_755f59de-471b-4aaa-95d8-8bf8ea5bd5b6\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0431\",\r\n        \"description\": \"App-consistent recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"0d11f5cd-9b6c-4184-aea9-d094944ee69e\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-06T04:47:14Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Repeated and multiple app-consistent tag generation failures on a replicating machine may be indicative of other issues, such as VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests.\",\r\n          \"details\": \"Apllication consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"App-consistent recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90075\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"App-consistent recovery tag generation for V2A-W2K16-201:2147483796 has failed.\",\r\n            \"possibleCauses\": \"App-consistent recovery point tagging failed.\",\r\n            \"recommendedAction\": \"\\n      This may be a transient failure. Azure Site Recovery will attempt generating an application consistent recovery point again in the next cycle. Check the latest application-consistent recovery point available for this replicated item from the Azure portal. If the latest available application consistent recovery point is very old and you are seeing repeated and multiple app-consistent tag generation failures on a replicating machine, this may be indicative of other issues, such as:\\n      1. VSS snapshot failures on Windows due to multiple VSS applications issuing snapshot requests. Check the VSS logs in event viewer on the source machine and fix any errors.\\n      2. Insufficient bandwidth to upload replication data either between the source machine and the process server, or the process server and the Azure storage account you are replicating to.Ensure you are replicating to the appropriate storage account tier based on the data change rate (churn) on the source machine and that the data change rate is within Azure Site Recovery supported limits for the storage tier you are replicating to.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-06T04:47:14Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751393110169781_fda7ec79-c0ec-43d6-8856-3499d5fcfed1\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:46:14.4606026Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402707177874_3c43b48e-d528-41a2-ac7f-e5118b7d5176\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to OK.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.7597933Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": []\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709577869_73d53630-7ac1-4750-8b52-726180a64b44\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Warning.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.5197938Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.5197938Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751402709877877_1448c675-495c-4029-b8e3-b80d592a3566\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH0007\",\r\n        \"description\": \"RPO threshold exceeded.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Warning\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:30:14.489793Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"78026\",\r\n            \"summaryMessage\": \"RPO threshold exceeded\",\r\n            \"errorMessage\": \"RPO has exceeded the configured threshold for source disks 'Disk0, Disk1'.\",\r\n            \"possibleCauses\": \"\\n      The RPO for a replicating machine may be impacted if:\\n      1. The machine is shutdown, the Mobility service components on the machine is not running, or the machine doesn't have network connectivity to the process server.\\n      2. The replicating machine is unable to upload changes fast enough to the process server, or the replicating machine has been flow controlled/throttled by the process server, thereby causing the machine to go into a non-data replication mode.\\n      3. Recovery tag generation failures on the replicating machine.\\n      4. The process server is unable to upload changes fast enough to the target/log Azure storage account (or the master target server if replicating to an on-premises site). This in turn can happen due to network connectivity issues /glitches or insufficient network throughput/bandwidth between the process server and the Azure log/target storage account.\\n      5. Storage IOPs/throughput limits are being hit on the log/target storage account resulting in reduced end to end upload throughput from the process server to the log/target storage account in Azure.\\n    \",\r\n            \"recommendedAction\": \"\\n      1. If there are other errors for the replicating machine, resolve them first.\\n      2. See the list of recent events for the replicating machine that may be impacting the RPO of the machine by going to the events section of the recovery services vault. If there are any such events, resolve them.\\n      3. Ensure that you have sufficient network bandwidth between the process server and the log/target Azure storage account (or master target server if replicating to on-premises site) to upload replication data, and that you are replicating to the appropriate tier of storage based on the data change rate characteristics of the replicating machine. Use the ASR deployment planner (https://aka.ms/asr-v2a-deployment-planner) to estimate the necessary network bandwidth requirements and the appropriate tier of storage to replicate to.\\n\\n      Refer to the article https://aka.ms/asr-v2a-rpo-exceeded , to learn how to troubleshoot replication issues.\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:30:14.489793Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751409564775807_445459bf-3a00-44d5-8969-d656c4a3a5c9\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:18:49Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K19-202:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K19-202(10.150.109.253)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K19-202:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T02:18:49Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412296711530_37aaa43f-54a0-475b-97e6-f84a1fd5fc0a\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.8064277Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.8064277Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412297111553_85383516-697b-41ca-b19d-3cd2ffe13933\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K19-202\",\r\n        \"affectedObjectCorrelationId\": \"ca44c47d-c6c0-4629-9821-8201881f8f85\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.7664254Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K19-202 (10.150.109.253) (Disk0) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.7664254Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300111488_e37e7c5b-69ea-4926-baa7-c12da89dd04c\",\r\n      \"properties\": {\r\n        \"eventCode\": \"SRSVMHealthChanged\",\r\n        \"description\": \"Replication health changed to Critical.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4664319Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4664319Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/VmMonitoringEvent;9090751412300661518_ad011b6c-41e1-48e3-be81-8e2e269b43b8\",\r\n      \"properties\": {\r\n        \"eventCode\": \"InMageCommon_ECH00014\",\r\n        \"description\": \"No replication progress in last 60 minutes.\",\r\n        \"eventType\": \"VmHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"00187181-233a-449a-b393-c66cd4689a7a\",\r\n        \"severity\": \"Critical\",\r\n        \"timeOfOccurrence\": \"2021-04-05T02:14:15.4114289Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": null,\r\n          \"category\": null,\r\n          \"component\": null,\r\n          \"correctiveAction\": null,\r\n          \"details\": null,\r\n          \"summary\": null,\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90078\",\r\n            \"summaryMessage\": \"No replication progress in 60 minutes\",\r\n            \"errorMessage\": \"Replication for V2A-W2K16-201 (10.150.108.22) (Disk0, Disk1) hasn't progressed in the last 60 minutes.\",\r\n            \"possibleCauses\": \"\\n      Replication may not be progressing due to​\\n      1. Network connectivity issues between the process server and the log/target Azure storage account (or master target server if replicating back to an on-premises site)​\\n      2. The Azure subscription of the target storage account has been disabled.​\\n    \",\r\n            \"recommendedAction\": \"\\n      1. Ensure that there is network connectivity between the process server and the log/target Azure storage account (or master-target server if replicating back to an on-premises site.)​\\n      2. If Identity is enabled on the Recovery Services Vault, please make sure the log/target Azure storage account has the necessary permissions to access the storage account.\\n          a) Go to your Storage account -> Access Control (IAM).\\n          b) Add the below role-assignments (for ARM based storage account) to the Recovery services vault.\\n            1) \\\"Contributor\\\" and,\\n            2) \\\"Storage Blob Data Contributor\\\" for Standard storage or \\\"Storage Blob Data Owner\\\" for Premium storage\\n      3. Ensure that the \\\"Microsoft Azure Recovery Services Agent\\\", and \\\"InMage Scout Vx Agent - Sentinel/Outpost\\\" services are running on the process server machine. Try restarting these services on the process server.​\\n\\n      Refer to the article https://aka.ms/asr-v2a-replication-not-progressing , to learn how to troubleshoot replication issues.​\\n      If the issue persists, contact support.​\\n    \",\r\n            \"creationTimeUtc\": \"2021-04-05T02:14:15.4114289Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    },\r\n    {\r\n      \"name\": \"AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"type\": \"Microsoft.RecoveryServices/vaults/replicationEvents\",\r\n      \"id\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationEvents/AgentMonitoringEvent;9090751422754775807_73b5ad18-9b80-4824-958b-1e9a986934fa\",\r\n      \"properties\": {\r\n        \"eventCode\": \"EA0430\",\r\n        \"description\": \"Recovery point tagging failed.\",\r\n        \"eventType\": \"AgentHealth\",\r\n        \"affectedObjectFriendlyName\": \"V2A-W2K16-201\",\r\n        \"affectedObjectCorrelationId\": \"e1a52cf6-cdc5-4a8c-913b-0447da3b024d\",\r\n        \"severity\": \"OK\",\r\n        \"timeOfOccurrence\": \"2021-04-05T01:56:50Z\",\r\n        \"fabricId\": \"/Subscriptions/b8aef8e1-37df-4f17-a537-f10e183c8eca/resourceGroups/PwsTestRG/providers/Microsoft.RecoveryServices/vaults/PwsTestVault/replicationFabrics/9a60d28b65543435e52e1b81073c6a3aca7dd5d00d201e49fbe4e86848c495ed\",\r\n        \"providerSpecificDetails\": {\r\n          \"instanceType\": \"InMageAzureV2\",\r\n          \"eventType\": \"Agent health\",\r\n          \"category\": \"Agent Has Logged Alerts\",\r\n          \"component\": \"Agent\",\r\n          \"correctiveAction\": \"This may be a transient failure. Check back after sometime.\",\r\n          \"details\": \"Crash consistency command failed. Command issued: \\\\\\\"C:\\\\\\\\Program Failed nodes: V2A-W2K16-201:2147483796 ExitCode: 2147483796\\\\n on V2A-W2K16-201(10.150.108.22)\",\r\n          \"summary\": \"Recovery point tagging failed.\",\r\n          \"siteName\": \"PwsTestCS\"\r\n        },\r\n        \"eventSpecificDetails\": null,\r\n        \"healthErrors\": [\r\n          {\r\n            \"errorSource\": null,\r\n            \"errorType\": null,\r\n            \"errorLevel\": null,\r\n            \"errorCategory\": \"Replication\",\r\n            \"errorCode\": \"90074\",\r\n            \"summaryMessage\": \"\",\r\n            \"errorMessage\": \"Recovery tag generation for V2A-W2K16-201:2147483796 has failed 3 times consecutively. \",\r\n            \"possibleCauses\": \"Recovery point tagging failed.\",\r\n            \"recommendedAction\": \"This may be a transient failure. Azure Site Recovery will attempt generating an recovery tag again in the next cycle. Check the latest recovery point available for this replicated item from the Azure portal. If the latest available recovery point is older than an hour and you are seeing repeated and multiple recovery tag generation failures on a replicating machine, this may be indicative of other issues, contact support.\",\r\n            \"creationTimeUtc\": \"2021-04-05T01:56:50Z\",\r\n            \"recoveryProviderErrorMessage\": null,\r\n            \"entityId\": null,\r\n            \"customerResolvability\": \"NotAllowed\"\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  ],\r\n  \"nextLink\": null\r\n}",
      "StatusCode": 200
    }
  ],
  "Names": {},
  "Variables": {
    "SubscriptionId": "b8aef8e1-37df-4f17-a537-f10e183c8eca"
  }
}